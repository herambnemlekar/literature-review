# High Level Planning Literature

***Terminology:*** *Segments = Options = Skills = Controllers = Behaviours = Motion Primitives*

## Symbolic Planning

1. [N. Gopalan, M. desJardins, M. Littman, J. MacGlashan, S. Squire, S. Tellex, J. Winder, and L. Wong, "Planning with abstract Markov decision
processes," In ICAPS, 2017.](http://h2r.cs.brown.edu/wp-content/uploads/2016/05/gopalan16.pdf)

1. [B. Ames, A. Thackston, and G.D. Konidaris. Learning Symbolic Representations for Planning with Parameterized Skills. Proceedings of the 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems, October 2018.](http://irl.cs.brown.edu/pubs/param_skill_symbols.pdf)

1. [S. James, B. Rosman, and G.D. Konidaris. Learning to Plan with Portable Symbols. In the ICML/IJCAI/AAMAS 2018 Workshop on Planning and Learning, July 2018.](https://cs.brown.edu/~gdk/pubs/pal18_portable_symbols.pdf)

1. [**G.D. Konidaris, L.P. Kaelbling, and T. Lozano-Perez. From Skills to Symbols: Learning Symbolic Representations for Abstract High-Level Planning. Journal of Artificial Intelligence Research 61, pages 215-289, January 2018.**](http://irl.cs.brown.edu/pubs/orig_sym_jair.pdf)

1. [**Bottom-Up Learning of Object Categories, Action Effects and Logical Rules: From Continuous Manipulative Exploration to Symbolic Planning (2015)**](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7139553)
	* The robot can execute front poke, side poke, top poke, pick, release, and stack actions.
	* The continuous world state consists of continuous object and robot states. Every object can be represented by a collection of effect categories (called object category) obtained by executing the 6 actions. The discrete world state consists of object categories and relations between objects.
	* By executing actions on single objects, effect categories are obtained for each action by unsupervised clustering. Mapping from object features to effect categories is learned by training multi-category classifiers for each action. Object categories can be predicted, given object features.
	* For obtaining effect categories of stack action, an exploratory action (poke) was required clearly understand object relations. Logical rules for stacking are generated by decision tree rule learning methods using object categories and relations.
	* *(Pending- Experiment Description)*

1. [**E. Ugur et al, "Staged Development of Robot Skills: Behavior Formation, Affordance Learning and Imitation with Motionese", IEEE Transactions on Autonomous Mental Development, vol. 7, (2), pp. 119-139, 2015.**](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7094253&tag=1)

1. D. Kent et al, "Learning Real-World Sequential Decision Tasks with Abstract Markov Decision Processes and Demonstration-Guided Exploration", RSS Workshop on Learning from Demonstrations for High Level Robotic Tasks, 2018.
	* Objects are instanstiated by assigning attributes to object's class. State vector contains only relations. Transition function maps from relations to relations. Sparse reward defined at sucess state and small negetive reward for each other state. AMDP has a projection function. Hierarchy specified by hand.
	* 

1. [G.D. Konidaris, L.P. Kaelbling, and T. Lozano-Perez. Symbol Acquisition for Probabilistic High-Level Planning. In Proceedings of the Twenty Fourth International Joint Conference on Artificial Intelligence, pages 3619-3627, July 2015.](https://cs.brown.edu/~gdk/pubs/sym-prob.pdf)

1. [G.D. Konidaris, L. Kaelbling and T. Lozano-Perez. Constructing Symbolic Representations for High-Level Planning. In Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence, pages 1932-1940, July 2014.](https://cs.brown.edu/~gdk/pubs/orig_sym_aaai.pdf)


1. [N. Jetchev, T. Lang, and M. Toussaint, "Learning Grounded Relational Symbols from Continuous Data for Abstract Reasoning"](https://ipvs.informatik.uni-stuttgart.de/mlr/papers/13-jetchev-ICRAws.pdf)

1. [E. Ugur, and J. Piater, "Bottom-up learning of object categories, action effects and logical rules: From continuous manipulative exploration to symbolic planning".](https://ieeexplore-ieee-org.ezproxy.wpi.edu/stamp/stamp.jsp?tp=&arnumber=7139553)

1. [E. Ugur, and J. Piater, "Refining discovered symbols with multi-step interaction experience".](https://ieeexplore-ieee-org.ezproxy.wpi.edu/stamp/stamp.jsp?tp=&arnumber=7363477)

1. [C. Belta, A. Bicchi, M. Egerstedt, E. Frazzoli, E. Klavins, and G. Pappas, "Symbolic planning and control of robot motion [Grand Challenges of Robotics]"](https://ieeexplore-ieee-org.ezproxy.wpi.edu/stamp/stamp.jsp?tp=&arnumber=4141034)


1. [M. Ghallab, D. Nau, and P. Traverso, "Automated  planning:  theory  and  practice," Morgan Kaufmann, 2004.](https://ebookcentral-proquest-com.ezproxy.wpi.edu/lib/wpi/detail.action?docID=333985)
	* [Lecture Slides](https://www.cs.umd.edu/~nau/cmsc421/planning.pdf)
	* [Binary Decision Diagrams](https://en.wikipedia.org/wiki/Binary_decision_diagram)

1. [PDDL](http://www.cs.cmu.edu/~mmv/planning/readings/98aips-PDDL.pdf)
	* Inspired from [ADL](https://en.wikipedia.org/wiki/Action_description_language) (updated STRIPS) and [UCPOP](http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=1C41DF9F8B7F171D5BA114A3BCEA1184?doi=10.1.1.45.5549&rep=rep1&type=pdf) (a planner for ADL)
	* Additions to STRIPS - 1) Negated preconditions, 2) Conditional effects, 3) Domain axioms, 4) Type specification of objects and 5) Hierarchial actions
	* Axioms define relationships between propositions
	* Closed world assumption. Actions are deterministic and instantaneous.
	* Propositional logic (as opposed to first order in ADL and STRIP)?
	* [PDDL vs ADL vs STRIPS](http://www.cogsys.wiai.uni-bamberg.de/teaching/ws0405/s_planning/slides/Introduction_AI_Planning_addon.pdf)

1. R. Fikes, and N. Nilsson, "STRIPS: a new approach to the application of theorem proving to problem solving," Artificial Intelligence, 2, 189–208, 1971
	* 1) inds a sequence of operators - In a space of world models - to transform an initial world model to a model in which a given goal formula can be proven true. 2) World model is arbitrary collection of [*first order predicate calculus*](https://en.wikipedia.org/wiki/First-order_logic) formulas. 3) Applies [*resolution*](https://en.wikipedia.org/wiki/Resolution_(logic)) rule for proving a first order formula is unsatisfiable (looks something like the subgoal condition). Uses [*means end analysis*](https://en.wikipedia.org/wiki/Means-ends_analysis) to reach goal-satisfying model
	* Closed world assumption- unmentioned literals are false
	* Primary application - Re-arranging objects and navigation

1. ADL vs STRIPS
	* STRIPS allows only positive literals,ADL can support both positive and negative literals. Eg: STRIPS- Rich ∧ Beautiful, ADL- ¬Poor ∧ ¬Ugly
    * STRIPS- closed-world assumption. ADL- Open world assumption (unmentioned literals are unknown)
    * STRIPS we only can find ground literals in goals. For instance, Rich ∧ Beautiful. In ADL we can find quantified variables in goals. Eg: ∃x At(P1, x) ∧ At(P2, x) is the goal of having P1 and P2 in the same place.
    * STRIPS- goals are conjunctions (e.g. Rich ∧ Beautiful). ADL, goals may involve conjunctions and disjunctions (e.g. Rich ∧ (Beautiful ∨ Smart)).
    * ADL conditional effects are allowed
    * STRIPS does not support equality. In ADL, the equality predicate (x = y ) is built in.
    * STRIPS does not have support for types, while in ADL it is supported (for example, the variable p : Person).


## Other High Level Learning

2. E. Carpio, M. Clark-Turner, P. Gesel, and M. Begum, "Leveraging Temporal Reasoning for Policy Selection in Learning from Demonstration".


## Skill Acqusition

3. [**G.D. Konidaris. Autonomous Robot Skill Acquisition. PhD Thesis, Department of Computer Science, University of Massachusetts Amherst, May 2011.**](http://irl.cs.brown.edu/pubs/rsa_thesis.pdf)
	* **Uses State-based Transitions Auto-Regressive Hidden Markov Model (STARHMM) to capture phase transitions (subgoals) and learn motor primitives for each subtask, refining it by policy search. Value function approach is for sequencing of the motor primitives.**
	* 
	* **Given only two demonstrations of bimanual grasping, the robot decomposed the task into five phases, and it learned a library of motor primitives. This was generalizable to different objects and locations.**

3. [**G. Konidaris et al, "Autonomous skill acquisition on a mobile manipulator", 2011.**](http://irl.cs.brown.edu/pubs/arsa-aaai.pdf)
	* **Skill acquisition consists of creating option, defining start and end conditions, determining appropriate abstraction and learning the policy. Option abstractions are usually learned from data, selected from a library or inferred from structure of factored MDP. CST uses library of abstractions. Trajectory is segmented (each segment is a skill) when an important abstraction changes or motions becomes too complex to approximate. The paper aims to learn skills that are transferable to other (similar) tasks.**
	* An option's initiation set is obtained using a classifier. The states that are in that segment are used as positive samples, and states that are not are used as negetive samples. Each option's termination condition is the initiation set of the next segment (skill). Skills from multiple demonstrations are merged if they are likely to be represented by the same value function.
	* The U-bot hand navigation controllers to reach a target object. It had 7 manipulation motions- withdrawn, extended, extended and left, right, up, down and outward. Orientation controller was not learned as it was crucial for safe navigation.
	* Abstractions were provided for each robot hand/body and target pair in terms of distance. Also body to wall distance was used. Robot learned task model as MDP. The value functions were initialzed to zero and time cost was used. Each acquired skill involved fitting a spline to the solution trajectory for identifying waypoints.
	* Future work: Build the abstractions in real-time. Manage skills (generalize or restric their use) to reduce complexity.

3. [**G. Konidaris et al, "CST: Constructing Skill Trees by Demonstration", 2011.**](http://cs.brown.edu/people/gdk/pubs/cst-ws.pdf)
	* **CST segments a demonstration trajectory into a chain of skills, where each skill has a goal (reach a configuration from which the next skill could be executed) and is assigned an abstraction from a library.  Skills  can be improved  by a policy learning algorithm. Chains from multiple demonstration are merged into a skill tree.**
	* Segmentation is performed based on linear value function approximation, i.e. a weighted sum of [Fourier basis functions](http://irl.cs.brown.edu/pubs/fourier-msrl.pdf) for each option. The sequentially received data is segmented by [maximum a posteriori (MAP) changepoint detection](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.63.8413&rep=rep1&type=pdf).
	* An abstraction is defined by a pair of functions- State Abstraction, maps the entire state space to a smaller state space (Sm), and Motor Abstraction, maps the full action space to a smaller action space (Am). Usually these are just subsets. When using an abstraction, the sensor input is filtered through the State Abstraction and its policy maps Sm to Am.
	* The MAP method is as follows: Every abstraction is a model q, with priors p(q). For a particular segment i to j, P(i,j,q) can be obtained. A HMM is formed with q as hidden states. Given transition probabilities from one model to other {g(j-i-1)p(q~j~) where g(i to j) is [probability mass function](https://en.wikipedia.org/wiki/Probability_mass_function)} and sensor data, the most likely path through hidden states is extimated by the [Viterbi algorithm](http://cecas.clemson.edu/~ahoover/ece854/refs/Gonze-ViterbiAlgorithm.pdf). Most changepoint probability will be close to zero. They can be discarded by Stratified Optimal Resampling algorithm. The trajectory is segmented such that the return estimate is represented by a piecewise linear value function where each segment is defined over a single abstraction.
	* Two chains are merged if the end states (goal or initiation set of a skill) of corresponding segments are same.

3. **G. Konidaris et al, "Robot learning from demonstration by constructing skill trees", The International Journal of Robotics Research, vol. 31, (3), pp. 360-375, 2012.**

3. [L. Riano, and T. M. McGinnity, "Automatically composing and parameterizing skills by evolving Finite State Automata".](https://www-sciencedirect-com.ezproxy.wpi.edu/science/article/pii/S0921889012000036)

## High + Low Level Planning
1.   L. Karlsson, J. Bidot, F. Lagriffoul, A. Saffiotti, U. Hillenbrand, and F. Schmidt, “Combining Task and Path Planning for a Humanoid Two-arm Robotic System,” TAMPRA, 2012.

1.  C. Dornhege, P. Eyerich, T. Keller, S. Trug, M. Brenner, and B. Nebel, “Semantic Attachments for Domain-Independent Planning Systems,” ICAPS, 2009.

1.  L. P. Kaelbling and T. Lozano-Pérez, “Hierarchical task and motion planning in the now,” in Proc. Conf. IEEE ICRA, 2011.

1.  F. Lagriffoul, L. Karlsson, J. Bidot, and A. Saffiotti, “Combining task and motion planning is not always a good idea,” in RSS  Workshop on Combined Robot Motion Planning and AI Planning for Practical Applications, 2013.

## Reinforcement Learning

[Resources](https://www.quora.com/What-are-the-best-resources-to-learn-Reinforcement-Learning)

### Hierarchial Reinforcement Learning
2. [**O. Kroemer et al, "Towards learning hierarchical skills for multi-phase manipulation tasks," in 2015, . DOI: 10.1109/ICRA.2015.7139389.**](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7139389)

2. [**B. Hayes and B. Scassellati, "Autonomously constructing hierarchical task networks for planning and human-robot collaboration," in 2016, . DOI: 10.1109/ICRA.2016.7487760.**](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7487760)

### Inverse Reinforcement Learning
2. [How does one learn a reward function in Reinforcement Learning?](https://www.quora.com/How-does-one-learn-a-reward-function-in-Reinforcement-Learning-RL)

2. [**Andrew Ng and Stuart Russell, "Algorithms for Inverse Reinforcement Learning", UC Berkely.**](http://ai.stanford.edu/~ang/papers/icml00-irl.pdf) 

## Other methods for symbolic representation and planning
1. [Inducing probabilistic context-free grammars for the sequencing of movement primitives](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8460190)
	- Assumptions: 1) We assume each segment of the observed sequences to be a sample from an underlying library of movement primitives. 2) Any produced sequence has to result in a continuous trajectory inside the state space. By restricting the operators to only produce valid grammars.
	- Definitions: 1) A probabilistic context-free grammar (PCFG) is a 4-tuple G = (A,V,R,S), where A is a set of terminals, V is a set of non terminals, S is a set of starting symbols and R = {(A,R,ρ)| A∈V} is a set of production rules. 2) Learning formal grammars from sequences of terminalsis referred to as grammar induction.
	- PCFG for movement primitives: 1) Θ is set of primitives and D is set of labelled demonstrations. 2) define the set of terminals as the set of primitives A = Θ i.e. primitives are immutable, implying that the search space consists of grammars that only differ in S, V or R. 3) Each grammar is a node in the grammar space, while the directed edges between nodes are defined by operators. Operators manipulate the rule set R of a grammar G to create a new grammar G′. 4) Posterior optimization- G* = argmax p(G|D) = argmax p(D|G) p(G). This is the total probability of the set of demonstrations D given a grammar G. 5) p(D|G) = ∏ p(d|G). 6) p(d|G)= (1/|parse (d,G)|) ∑_{τ∈parse(d,G)} ∏_{(A,r,ρ)∈τ} ρ. This is the average probability over each tree that produces the demonstartion d from grammar G. 7) Grammar prior is joint distribution over grammar probabilities ρ_G = {ρ_A | A∈V} and grammar structure G_R = {ρ_A | A∈V}. 
	- Traversing grammar space: 1) We define a domain Ω_op for each operator op ∈ O, that op can act on. 2) After creating new grammar G', parameters have to be recomputed using Inside-Outside algorithm. 3) Every sequence produced by G has to guarantee a smooth, continuous trajectory within the statespace of the MPs. We restrict the grammar space G to only contain grammars that fulfill this continuity requirement. The restriction is achieved by limiting the domain Ω_op of each operator, such that if grammar G fulfills the continuity requirement any grammar G′resulting from an application of op on G also fulfills the requirement.
	- Summary: Demonstrations are segmented to get primitives. Primitives are immutable. We start from a grammar prior. Each grammar is defined by terminals (A), non-terminals (V), rules (R) and starting symbols (S). Each rule has productions, each of which is a sequence. We apply operators to rules of the previous grammar to get new grammar. The operators are applied such that the new grammar can fit to all demonstrations. The operator changes the rules and non-terminals such that the new rules have new productions (sequences). The rules define how a tree will expand from the start symbols.

1. [State representation learning (SRL) for control: An overview](https://arxiv.org/pdf/1802.04181.pdf)
	- SRL is a case of feature learning where the featuresto learn are low dimensional, evolve through time, and are influenced by actions or interactions. SRL learns a mapping φ of the history of observation to the current state s_t=φ(o_{1:t}). 
	- Difference: reduces the state space from S∈R^d to some S∈R^d' where d'<< d. While symbolic representation reduces state space to a set S∈Ω where |Ω|∈R.

1. [Learning and reasoning with action-related places for robust mobile manipulation](https://arxiv.org/pdf/1401.4599.pdf)
	- Abstract: 1) Represent robot locations as a collection of positions each associated with a probability that a manipulation action will succeed from it. 2) ARPlace generated through xperience-based learning. 3) ARPlace updated based on new task information. 4) Least-commitment approach. Transformational planner optimizes symbolic plans.
	- ARPlace: The probability of successful manipulation given the current estimate of object position (and robot positions).
	- Planning: 1) RPL
	- Similarity to symbol intersections: 1) Because grasping both cups at once from a single position is much more efficient than approaching two locations, the robot merges the two ARPlaces for each cup into one ARPlace representing the probability of successfully grasping both cups from a single position.
	- Difference to symbol intersections: 1) Requires 'flaw detection' to identify better options. Pre-defined flaws. Additional information like collisions, supporting planes and other concepts of the world is needed. 2) For RPL, expressions and objects need to be annotated
	- Similarity to parameterized symbols: 1) Learn SVM for a task parameterization. Use Point Distribution Models to generalize over SVMs for several task parameterizations. Choose 20 landmarks (points) on the classification boundary corresponding to each of the 16 object positions. Arrange the x and y of each point in a 40x16 matrix where each column is 20 x and 20 y of the landmarks for an object position. Represent each boundary as mean + (k x variance). Use regression to interpolate over parameters like k.
	- Limitation: 1) Is it assumed that moving close to an initial ARPlace estimate will result in information gain leading to a better ARPlace estimate? No explicit modelling of information seeking behaviour. 2) To collect data the robot repeatedly executes a specific action sequence. 3) Features w.r.t. table? Rather should be w.r.t. robot. As everything would be consistent w.r.t. robot. 4) What if the grasps are failed during data collection due to some other factor and not due to robot position? 5) The boundaries of SVMs may not be regression-able. We don't know that an interpolation would actually work in real world.

1. [CRAM—A Cognitive Robot Abstract Machine for everyday manipulation in human environments](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5650146)
	-  powerful  softwaretools  that  enable  programmers  to  effectively  and  efficientlyimplement higher-level capabilities such as learning, knowl-edge  processing,  and  action  planning  into  the  robot  controlprograms  to  produce  more  flexible,  reliable  and  efficientbehavior.
	- CRAM   provides   a   language   for   programmingcognition-enabled  control  systems.  This  language  includesdata  structures,  primitive  control  structures,  tools  and  li-braries  that  are  specifically  designed  to  enable  and  supportmobile  manipulation  as  well  as  cognition-enabled  control.
	- The  backbones  of  CRAM  are  the  CRAM  Plan  Lan-guage    (CPL)    and    the    knowledge    processing    system KNOWROB. CPL helps to not  only  execute  their  control  programs,  but  also  to  reasonabout  and  manipulate  them  —  even  during  their  execution.  KNOWROBis a first-orderknowledge  representation  based  on  description  logics  andprovides  specific  mechanisms  and  tools  for  action-centricrepresentation,  for  the  automated  acquisition  of  groundedconcepts  through  observation  and  experience,  for  reasoningabout  and  managing  uncertainty,  and  for  fast  inference  —knowledge processing features that are particularly necessaryfor autonomous robot control.
	- in contrast to layeredarchitectures that need to abstract away from these low-leveldata structures, in CPL they are still available to the completeprogram and can be used for decision making
	- CRAM is designed to be lightweight by  using  and  extending  the  Common  Lisp  languageand  by  using  existing  Lisp  compilers  rather  than  buildinga plan language with its own interpreter, as it has been donefor  RPL. 
	- programmer  has  to  satisfy  restrictionsand  coding  conventions  only  for  those  components  that  areneeded. For example, plans can be written in a simpler wayand  do  not  have  to  contain  annotations  that  would  allow  areasoner to reconstruct goals, the structure of sub-plans andthe belief state if the robot is not supposed to reason aboutthe course of action anyways.
	- Limitations:  program-mers define how the truth value of particular predicates canbe computed on demand from the respective data structures.We call this kind of predicatescomputable predicates.
	- Useful comments in paper: The biggest problem with these architectures is that it is hard to find abstractions that are suitable for planning in time: On the one hand, they must reduce complexity enough to make planning feasible, but on the other hand, they should not abstract away information that is necessary for decision making
