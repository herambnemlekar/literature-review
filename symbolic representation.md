## Other methods for symbolic representation and planning
1. [Inducing probabilistic context-free grammars for the sequencing of movement primitives](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8460190)
	- Assumptions: 1) We assume each segment of the observed sequences to be a sample from an underlying library of movement primitives. 2) Any produced sequence has to result in a continuous trajectory inside the state space. By restricting the operators to only produce valid grammars.
	- Definitions: 1) A probabilistic context-free grammar (PCFG) is a 4-tuple G = (A,V,R,S), where A is a set of terminals, V is a set of non terminals, S is a set of starting symbols and R = {(A,R,ρ)| A∈V} is a set of production rules. 2) Learning formal grammars from sequences of terminalsis referred to as grammar induction.
	- PCFG for movement primitives: 1) Θ is set of primitives and D is set of labelled demonstrations. 2) define the set of terminals as the set of primitives A = Θ i.e. primitives are immutable, implying that the search space consists of grammars that only differ in S, V or R. 3) Each grammar is a node in the grammar space, while the directed edges between nodes are defined by operators. Operators manipulate the rule set R of a grammar G to create a new grammar G′. 4) Posterior optimization- G* = argmax p(G|D) = argmax p(D|G) p(G). This is the total probability of the set of demonstrations D given a grammar G. 5) p(D|G) = ∏ p(d|G). 6) p(d|G)= (1/|parse (d,G)|) ∑_{τ∈parse(d,G)} ∏_{(A,r,ρ)∈τ} ρ. This is the average probability over each tree that produces the demonstartion d from grammar G. 7) Grammar prior is joint distribution over grammar probabilities ρ_G = {ρ_A | A∈V} and grammar structure G_R = {ρ_A | A∈V}. 
	- Traversing grammar space: 1) We define a domain Ω_op for each operator op ∈ O, that op can act on. 2) After creating new grammar G', parameters have to be recomputed using Inside-Outside algorithm. 3) Every sequence produced by G has to guarantee a smooth, continuous trajectory within the statespace of the MPs. We restrict the grammar space G to only contain grammars that fulfill this continuity requirement. The restriction is achieved by limiting the domain Ω_op of each operator, such that if grammar G fulfills the continuity requirement any grammar G′resulting from an application of op on G also fulfills the requirement.
	- Summary: Demonstrations are segmented to get primitives. Primitives are immutable. We start from a grammar prior. Each grammar is defined by terminals (A), non-terminals (V), rules (R) and starting symbols (S). Each rule has productions, each of which is a sequence. We apply operators to rules of the previous grammar to get new grammar. The operators are applied such that the new grammar can fit to all demonstrations. The operator changes the rules and non-terminals such that the new rules have new productions (sequences). The rules define how a tree will expand from the start symbols.

1. [State representation learning (SRL) for control: An overview](https://arxiv.org/pdf/1802.04181.pdf)
	- SRL is a case of feature learning where the featuresto learn are low dimensional, evolve through time, and are influenced by actions or interactions. SRL learns a mapping φ of the history of observation to the current state s_t=φ(o_{1:t}). 
	- Difference: reduces the state space from S∈R^d to some S∈R^d' where d'<< d. While symbolic representation reduces state space to a set S∈Ω where |Ω|∈R.

1. [Learning and reasoning with action-related places for robust mobile manipulation](https://arxiv.org/pdf/1401.4599.pdf)
	- Abstract: 1) Represent robot locations as a collection of positions each associated with a probability that a manipulation action will succeed from it. 2) ARPlace generated through xperience-based learning. 3) ARPlace updated based on new task information. 4) Least-commitment approach. Transformational planner optimizes symbolic plans.
	- ARPlace: The probability of successful manipulation given the current estimate of object position (and robot positions).
	- Planning: 1) RPL
	- Similarity to symbol intersections: 1) Because grasping both cups at once from a single position is much more efficient than approaching two locations, the robot merges the two ARPlaces for each cup into one ARPlace representing the probability of successfully grasping both cups from a single position.
	- Difference to symbol intersections: 1) Requires 'flaw detection' to identify better options. Pre-defined flaws. Additional information like collisions, supporting planes and other concepts of the world is needed. 2) For RPL, expressions and objects need to be annotated
	- Similarity to parameterized symbols: 1) Learn SVM for a task parameterization. Use Point Distribution Models to generalize over SVMs for several task parameterizations. Choose 20 landmarks (points) on the classification boundary corresponding to each of the 16 object positions. Arrange the x and y of each point in a 40x16 matrix where each column is 20 x and 20 y of the landmarks for an object position. Represent each boundary as mean + (k x variance). Use regression to interpolate over parameters like k.
	- Limitation: 1) Is it assumed that moving close to an initial ARPlace estimate will result in information gain leading to a better ARPlace estimate? No explicit modelling of information seeking behaviour. 2) To collect data the robot repeatedly executes a specific action sequence. 3) Features w.r.t. table? Rather should be w.r.t. robot. As everything would be consistent w.r.t. robot. 4) What if the grasps are failed during data collection due to some other factor and not due to robot position? 5) The boundaries of SVMs may not be regression-able. We don't know that an interpolation would actually work in real world.

1. [CRAM—A Cognitive Robot Abstract Machine for everyday manipulation in human environments](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5650146)
	-  powerful  softwaretools  that  enable  programmers  to  effectively  and  efficientlyimplement higher-level capabilities such as learning, knowl-edge  processing,  and  action  planning  into  the  robot  controlprograms  to  produce  more  flexible,  reliable  and  efficientbehavior.
	- CRAM   provides   a   language   for   programmingcognition-enabled  control  systems.  This  language  includesdata  structures,  primitive  control  structures,  tools  and  li-braries  that  are  specifically  designed  to  enable  and  supportmobile  manipulation  as  well  as  cognition-enabled  control.
	- The  backbones  of  CRAM  are  the  CRAM  Plan  Lan-guage    (CPL)    and    the    knowledge    processing    system KNOWROB. CPL helps to not  only  execute  their  control  programs,  but  also  to  reasonabout  and  manipulate  them  —  even  during  their  execution.  KNOWROBis a first-orderknowledge  representation  based  on  description  logics  andprovides  specific  mechanisms  and  tools  for  action-centricrepresentation,  for  the  automated  acquisition  of  groundedconcepts  through  observation  and  experience,  for  reasoningabout  and  managing  uncertainty,  and  for  fast  inference  —knowledge processing features that are particularly necessaryfor autonomous robot control.
	- in contrast to layeredarchitectures that need to abstract away from these low-leveldata structures, in CPL they are still available to the completeprogram and can be used for decision making
	- CRAM is designed to be lightweight by  using  and  extending  the  Common  Lisp  languageand  by  using  existing  Lisp  compilers  rather  than  buildinga plan language with its own interpreter, as it has been donefor  RPL. 
	- programmer  has  to  satisfy  restrictionsand  coding  conventions  only  for  those  components  that  areneeded. For example, plans can be written in a simpler wayand  do  not  have  to  contain  annotations  that  would  allow  areasoner to reconstruct goals, the structure of sub-plans andthe belief state if the robot is not supposed to reason aboutthe course of action anyways.
	- Limitations:  program-mers define how the truth value of particular predicates canbe computed on demand from the respective data structures.We call this kind of predicatescomputable predicates.